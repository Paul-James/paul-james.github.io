---
title: "Resume"
date: "May 8, 2018"
---

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.12/css/all.css" integrity="sha384-G0fIWCsCzJIMAVNQPfjH08cyYaUtMwjJwqiRKxxE/rx96Uroj1BtIQ6MLJuheaO9" crossorigin="anonymous">

# Paul James <img src='https://drive.google.com/uc?id=1emoFXopfp16h7imONGIK1h4mrGxK5uC0' align='right'/>
<br>

|  |  |  |  |  |
|:-:|:-:|:-:|:-:|:-:|:-:|
| <i class="fab fa-linkedin fa-2x"></i> | <i class="fab fa-r-project fa-2x"></i> | <i class="fab fa-twitter fa-2x"></i> | <i class="fas fa-at fa-2x"></i> | <i class="fas fa-phone fa-2x"></i> |
| [pauljames379](https://linkedin.com/in/pauljames379) | [pjames](https://paul-james.me/pjames) | [@PaulJames379](https://twitter.com/pauljames379) | paul.james379@gmail.com | +18653095874 |

---

## Summary

Accomplished __Data Scientist__ with extensive `R`, `SQL`, `ETL`, `Relational Database`, `Tableau`, and _`Hadoop Stack`_ experience.
Skilled in `data quality`, `data cleaning`, `statistical modeling`, and `predictive statistics`.
A team player with the ability to investigate and mine for results.
Effective communicator capable of succeeding in fast-paced environments with meticulous attention to detail.

## Areas of Expertise

| | | | |
|-|-|-|-|
| R and SQL | Data Mining | Machine Learning | Optimization |
| Linear/Logistic Regression | Problem Solving | Automated Reporting | Forecasting |
| Experimental Design | Data Warehousing | Process Control | Markdown

## Professional Experience

#### Data Scientist <i class="fas fa-code-branch"></i>

> __Nissan North America, Inc.__ • _2017-present_
>
> *
> *

> __PopHealthCare__ • _2016_
> * Leverage historical data, statistical modeling techniques, and machine learning to identify potentially high risk Medicare Advantage patients in order to manage their care and prevent adverse health outcomes.
> * Use SQL procedures, R database connectivity, and R programming to manipulate big data and simplify data processes into automated scripts.

> __PYA Analytics__ • _2015_
> * Wrote software to automatically generate reports that sped up the data collecting and report writing process from 2 weeks to 15 minutes, and eliminated 40 hours of labor per month.
> * Adept at using open-source software and tools (MySQL, PostgreSQL, R) to accomplish results completely and effectively; providing streamlined completion while reducing project timelines and costs.

#### Data Analyst <i class="far fa-chart-bar"></i>

> __PYA Analytics__ • _2014_
> * Provided an interactive dashboard solution to large ACOs giving power to decision makers regarding costs and use. The dashboard enabled decision makers to lower costs and improve quality of beneficiary care.
> * Provided data warehouse quality audit and cleaned datasets for use in proprietary survival analysis tools to evaluate US Marine Corp vehicle status and lifetime expectancy to aid in selection for missions.

## Education

`Bachelor of Science`
__`Business Administration: Business Analytics, Supply Chain Management`__
UNIVERSITY OF TENNESSEE – KNOXVILLE

---

## Projects and Accomplishments

__GOAL:__ _Develop a system for predicting which patients will be the most sick and costly in order to improve their health by managing their care._
__RESULT:__ Handed a disjoint and disparate process and and was told to automate and centralize it. In under a few months transformed this convoluted process into a streamlined one. It went from hundreds of steps that acted on billions of rows of data across multiple servers, to just a few steps that did the same thing. Not only did this process save over 2 days of work every few weeks, but the data manipulations behind the scenes are optimized and robust. The machine learning model (optimized random forest) is significantly better while being statistically sound and robust. Because of all these factors, the company was able to increase the number of clients they took on which increased the amount of money they made. I’m not privy to a quantitative financial impact, but I was told by the CFO, “We're really glad you’re here”.
__TOOLS USED:__ `R`, `SQL`, `SSIS`, `SQL Server`, `Powershell`, `Insurance Claims`, `Optimized Random Forest`, `Logistic Regression`

__GOAL:__ _Audit healthcare practice diagnosis coding. The more correct codes billed for, the more revenue received from CMS._
__RESULT:__ Reverse engineered a CMS (Centers for Medicare/Medicaid Services) population health scoring algorithm (called HCC) from SAS to R and used it to audit a group of physician practice's diagnosis coding (ICD9s) for the previous few years. Delivered audited population health scores and expected capitations which gave a clear picture of how their diagnosis coding practices were currently performing and where they needed to improve.
__TOOLS USED:__ `R`, `bash`, `SAS`, `Excel`, `CMS HCC algorithm`, `CMS Public Use files`, `ICD9`

__GOAL:__ _Improve ACO’s health system costs and use while at the same time help those same systems increase margins and revenue._
__RESULT:__ Implemented the CMMI Bundled Payments for Care Improvement initiative by performing claims data ETL, analysis, and reporting on LDS (CMS Limited Datasets) files for ACOs. Applied bundled payments metrics and algorithms to calculate past and present costs. Delivered a series of dynamic Tableau dashboards to show current, past, and trending costs/use by acute care provider, physician, diagnosis, and post-acute provider from high level aggregate views of the data to very low level line by line costs. This tool provides a clear picture of the total cost of care for a patient across the entire healthcare spectrum to aid in improving healthcare cooperation, cost, and quality. This tool gave these ACO’s a tangible path to save hundreds of thousands of dollars year over year.
__TOOLS USED:__ `R`, `Python`, `Postgres`, `SQL`, `Tableau`, `SparkR`, `CMS Public Use files`, `CMS Limited Dataset files`, `bash`

__GOAL:__ _Cleanse data and create tools for LOGCOM to quickly access ground vehicle health status, mobility, and expected lifetime._
__RESULT:__ Using a US government security clearance to access and analyze United States Marine Corps Logistics Command's (LOGCOM) Oracle big data databases, was in charge of warehouse data quality assessment, data cleaning, and preparing data extracts for use in statistical analysis and reporting (written report and Tableau dashboards). Quickly diagnosed data quality problems and found alternative solutions. Our team provided LOGCOM tools to analyze their worldwide fleet of ground vehicles to make data-driven decisions with regards to costs, counts, health, mobility, and expected lifespan.
__TOOLS USED:__ `R`, `Oracle`, `bash`, `VPN`, `advanced statistical modeling`, `Tableau`, `Tableau Server`, `SQL`

__GOAL:__ _Create an easy to reproduce data quality report for clinical hospital data and recommend quality metrics to save the world’s largest for-profit operator of healthcare facilities time and money from manually making the report every month._
__RESULT:__ Created an automated report-generating software. This software uses Excel to gather report requirements, then uses R, markdown, and R wrappers for JavaScript visualization libraries to automatically generate a well formatted, interactive HTML data quality report. This report is a mixture of text, tables, and visualizations. It's stand-alone (doesn't need to be hosted on a website) and portable. This beautiful report exceeded the healthcare operator's requirements and manipulated the data into summaries for use in Tableau. The automated report saves an employee at this company 1 week every month in data processing and putting this report together manually. Also performed statistical techniques to recommend quality metrics and limits for their clinical data.
__TOOLS USED:__ `R`, `Markdown`, `Dimple.js`, `DataTables.js`, `Excel`, `HTML`, `CSS`, `bash`

---
